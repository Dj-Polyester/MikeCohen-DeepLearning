{"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["# COURSE: A deep understanding of deep learning\n","## SECTION: Overfitting, cross-validation, regularization\n","### LECTURE: Splitting data into train, devset, test\n","#### TEACHER: Mike X Cohen, sincxpress.com\n","##### COURSE URL: udemy.com/course/dudl/?couponCode=202208"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"YeuAheYyhdZw"},"outputs":[],"source":["# import libraries\n","import numpy as np\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"q-YUb7pW19yy"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 11  12  13  14]\n"," [ 21  22  23  24]\n"," [ 31  32  33  34]\n"," [ 41  42  43  44]\n"," [ 51  52  53  54]\n"," [ 61  62  63  64]\n"," [ 71  72  73  74]\n"," [ 81  82  83  84]\n"," [ 91  92  93  94]\n"," [101 102 103 104]]\n"," \n","[False False False False False  True  True  True  True  True]\n"]}],"source":["### create fake dataset (same as in previous videos)\n","\n","fakedata = np.tile(np.array([1,2,3,4]),(10,1)) + np.tile(10*np.arange(1,11),(4,1)).T\n","fakelabels = np.arange(10)>4\n","print(fakedata), print(' ')\n","print(fakelabels)"]},{"cell_type":"markdown","metadata":{"id":"UhkvsJ6g6uXr"},"source":["# Using train_test_split"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8bxbHGkP7JW3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data size: (8, 4)\n","Devset data size: (1, 4)\n","Test data size: (1, 4)\n"," \n","Training data: \n","[[ 61  62  63  64]\n"," [ 91  92  93  94]\n"," [ 81  82  83  84]\n"," [ 21  22  23  24]\n"," [ 51  52  53  54]\n"," [ 71  72  73  74]\n"," [ 41  42  43  44]\n"," [101 102 103 104]]\n"," \n","Devset data: \n","[[31 32 33 34]]\n"," \n","Test data: \n","[[11 12 13 14]]\n"]}],"source":["# specify sizes of the partitions\n","# order is train,devset,#test\n","partitions = [.8,.1,.1]\n","\n","# split the data (note the third input, and the TMP in the variable name)\n","train_data,testTMP_data, train_labels,testTMP_labels = \\\n","                   train_test_split(fakedata, fakelabels, train_size=partitions[0])\n","\n","# now split the TMP data\n","split = partitions[1] / (1-partitions[0])\n","devset_data,test_data, devset_labels,test_labels = \\\n","              train_test_split(testTMP_data, testTMP_labels, train_size=split)\n","\n","\n","\n","\n","# print out the sizes\n","print('Training data size: ' + str(train_data.shape))\n","print('Devset data size: '   + str(devset_data.shape))\n","print('Test data size: '     + str(test_data.shape))\n","print(' ')\n","\n","# print out the train/test data\n","print('Training data: ')\n","print(train_data)\n","print(' ')\n","\n","print('Devset data: ')\n","print(devset_data)\n","print(' ')\n","\n","print('Test data: ')\n","print(test_data)"]},{"cell_type":"markdown","metadata":{"id":"EvUQFxSTV2SB"},"source":["# Splitting the data manually using numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUZcKNWsXg00"},"outputs":[],"source":["# partition sizes in proportion\n","partitions = np.array([.8,.1,.1])\n","print('Partition proportions:')\n","print(partitions)\n","print(' ')\n","\n","# convert those into integers\n","partitionBnd = np.cumsum(partitions*len(fakelabels)).astype(int)\n","print('Partition boundaries:')\n","print(partitionBnd)\n","print(' ')\n","\n","\n","# random indices\n","randindices = np.random.permutation(range(len(fakelabels)))\n","print('Randomized data indices:')\n","print(randindices)\n","print(' ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vre4YiQBZmjy"},"outputs":[],"source":["# select rows for the training data\n","train_dataN   = fakedata[randindices[:partitionBnd[0]],:]\n","train_labelsN = fakelabels[randindices[:partitionBnd[0]]]\n","\n","# select rows for the devset data\n","devset_dataN   = fakedata[randindices[partitionBnd[0]:partitionBnd[1]],:]\n","devset_labelsN = fakelabels[randindices[partitionBnd[0]:partitionBnd[1]]]\n","\n","# select rows for the test data\n","test_dataN   = fakedata[randindices[partitionBnd[1]:],:]\n","test_labelsN = fakelabels[randindices[partitionBnd[1]:]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbTLW0MkXg-V"},"outputs":[],"source":["# print out the sizes\n","print('Training data size: ' + str(train_dataN.shape))\n","print('Devset size: '        + str(devset_dataN.shape))\n","print('Test data size: '     + str(test_dataN.shape))\n","print(' ')\n","\n","# print out the train/test data\n","print('Training data: ')\n","print(train_dataN)\n","print(' ')\n","\n","print('Devset data: ')\n","print(devset_dataN)\n","print(' ')\n","\n","print('Test data: ')\n","print(test_dataN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaP1IQDrXhBc"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyORIyTWClz7YOKdhmp8/3fQ","collapsed_sections":[],"name":"DUDL_overfitting_trainDevsetTest.ipynb","provenance":[{"file_id":"1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg","timestamp":1616615469755},{"file_id":"1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK","timestamp":1616608248670}]},"kernelspec":{"display_name":"Python 3.10.7 ('datascience')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"35e756a73fced7604de0846138dbd2729bf77c54ea1471d9ad302cb6526e59eb"}}},"nbformat":4,"nbformat_minor":0}
