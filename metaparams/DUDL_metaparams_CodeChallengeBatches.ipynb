{"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["# COURSE: A deep understanding of deep learning\n","## SECTION: Metaparameters (activation, batch, optimizers)\n","### LECTURE: CodeChallenge: Minibatch size in the wine dataset\n","#### TEACHER: Mike X Cohen, sincxpress.com\n","##### COURSE URL: udemy.com/course/dudl/?couponCode=202208"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7U3TmybM4yMw"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10233/2969157509.py:23: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n","  display.set_matplotlib_formats('svg')\n"]}],"source":["### import libraries\n","\n","# for DL modeling\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader,TensorDataset\n","from sklearn.model_selection import train_test_split\n","\n","# for number-crunching\n","import numpy as np\n","import scipy.stats as stats\n","\n","# for dataset management\n","import pandas as pd\n","\n","# for timing computations\n","import time\n","\n","# for data visualization\n","import matplotlib.pyplot as plt\n","from IPython import display\n","display.set_matplotlib_formats('svg')"]},{"cell_type":"markdown","metadata":{"id":"2anVFzBXGdwH"},"source":["# Import and process the data"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0ohXIxzt4_U2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10233/649372525.py:14: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  data['boolQuality'][data['quality']>5] = 1\n"]}],"source":["# import the data\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n","data = pd.read_csv(url,sep=';')\n","data = data[data['total sulfur dioxide']<200] # drop a few outliers\n","\n","# z-score all columns except for quality\n","cols2zscore = data.keys()\n","cols2zscore = cols2zscore.drop('quality')\n","data[cols2zscore] = data[cols2zscore].apply(stats.zscore)\n","\n","# create a new column for binarized (boolean) quality\n","data['boolQuality'] = 0\n","# data['boolQuality'][data['quality']<6] = 0 # implicit in the code! just here for clarity\n","data['boolQuality'][data['quality']>5] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjvI_6Su5Gk-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"vGQd7xmM5Gns"},"source":["# Re-organize the data: train/test in DataLoaders"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"2kZ6YPe8Gav5"},"outputs":[],"source":["# convert from pandas dataframe to tensor\n","dataT  = torch.tensor( data[cols2zscore].values ).float()\n","labels = torch.tensor( data['boolQuality'].values ).float()\n","labels = labels[:,None] # transform to matrix"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bbf064xxGa_x"},"outputs":[],"source":["# use scikitlearn to split the data\n","train_data,test_data, train_labels,test_labels = train_test_split(dataT, labels, test_size=.1)\n","\n","# then convert them into PyTorch Datasets (note: already converted to tensors)\n","train_dataDataset = TensorDataset(train_data,train_labels)\n","test_dataDataset  = TensorDataset(test_data,test_labels)"]},{"cell_type":"markdown","metadata":{"id":"I7g0mivk5GqP"},"source":["# Now for the DL part"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"N0vAnQi9DNRa"},"outputs":[],"source":["# create a class for the model\n","\n","class ANNwine(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    ### input layer\n","    self.input = nn.Linear(11,16)\n","    \n","    ### hidden layers\n","    self.fc1 = nn.Linear(16,32)\n","    self.fc2 = nn.Linear(32,32)\n","\n","    ### output layer\n","    self.output = nn.Linear(32,1)\n","  \n","  # forward pass\n","  def forward(self,x):\n","    x = F.relu( self.input(x) )\n","    x = F.relu( self.fc1(x) ) # fully connected\n","    x = F.relu( self.fc2(x) )\n","    return self.output(x)"]},{"cell_type":"markdown","metadata":{"id":"IuCixgNfDMZS"},"source":["# Train the model"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"0XRPe56rGp2k"},"outputs":[],"source":["# a function that trains the model\n","\n","# global parameter\n","numepochs = 1000\n","\n","def trainTheModel():\n","\n","  # loss function and optimizer\n","  lossfun = nn.BCEWithLogitsLoss()\n","  optimizer = torch.optim.SGD(winenet.parameters(),lr=.01)\n","\n","  # initialize losses\n","  losses   = torch.zeros(numepochs)\n","  trainAcc = []\n","  testAcc  = []\n","\n","  # loop over epochs\n","  for epochi in range(numepochs):\n","\n","    # switch on training mode\n","    winenet.train()\n","\n","    # loop over training data batches\n","    batchAcc  = []\n","    batchLoss = []\n","    for X,y in train_loader:\n","\n","      # forward pass and loss\n","      yHat = winenet(X)\n","      loss = lossfun(yHat,y)\n","\n","      # backprop\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # loss from this batch\n","      batchLoss.append(loss.item())\n","\n","      # compute training accuracy for this batch\n","      batchAcc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n","    # end of batch loop...\n","\n","    # now that we've trained through the batches, get their average training accuracy\n","    trainAcc.append( np.mean(batchAcc) )\n","\n","    # and get average losses across the batches\n","    losses[epochi] = np.mean(batchLoss)\n","\n","    # test accuracy\n","    winenet.eval()\n","    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n","    with torch.no_grad(): # deactivates autograd\n","      yHat = winenet(X)\n","    testAcc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n","  \n","  # function output\n","  return trainAcc,testAcc,losses"]},{"cell_type":"markdown","metadata":{"id":"T_JKCpfe_CfC"},"source":["# Now for the experiment"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"oL7EqhYjGp51"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb Cell 14\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# create and train a model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m winenet \u001b[39m=\u001b[39m ANNwine()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m trainAcc,testAcc,losses \u001b[39m=\u001b[39m trainTheModel()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# store data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m accuracyResultsTrain[:,bi] \u001b[39m=\u001b[39m trainAcc\n","\u001b[1;32m/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb Cell 14\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m X,y \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   \u001b[39m# forward pass and loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   yHat \u001b[39m=\u001b[39m winenet(X)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   loss \u001b[39m=\u001b[39m lossfun(yHat,y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m   \u001b[39m# backprop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/polyester/Desktop/Programming/DL/DUDL_PythonCode/metaparams/DUDL_metaparams_CodeChallengeBatches.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n","File \u001b[0;32m~/.venv/datascience/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.venv/datascience/lib/python3.10/site-packages/torch/nn/modules/loss.py:714\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 714\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    715\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    716\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    717\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n","File \u001b[0;32m~/.venv/datascience/lib/python3.10/site-packages/torch/nn/functional.py:3150\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m   3148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[0;32m-> 3150\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target, weight, pos_weight, reduction_enum)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# This cell takes ~15 mins\n","\n","# range of batch sizes\n","batchsizes = 2**np.arange(1,10,2)\n","\n","# initialize output results matrices\n","accuracyResultsTrain = np.zeros((numepochs,len(batchsizes)))\n","accuracyResultsTest  = np.zeros((numepochs,len(batchsizes)))\n","comptime             = np.zeros(len(batchsizes))\n","\n","# test data doesn't vary by training batch size\n","test_loader = DataLoader(test_dataDataset,batch_size=test_dataDataset.tensors[0].shape[0])\n","\n","# loop over batch sizes\n","for bi in range(len(batchsizes)):\n","\n","  # start the clock!\n","  starttime = time.process_time()\n","\n","  # create dataloader object\n","  train_loader = DataLoader(train_dataDataset,\n","                          batch_size=int(batchsizes[bi]), shuffle=True, drop_last=True)\n","\n","  # create and train a model\n","  winenet = ANNwine()\n","  trainAcc,testAcc,losses = trainTheModel()\n","\n","  # store data\n","  accuracyResultsTrain[:,bi] = trainAcc\n","  accuracyResultsTest[:,bi]  = testAcc\n","\n","  # check the timer\n","  comptime[bi] = time.process_time() - starttime\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7mAB4utDMeh"},"outputs":[],"source":["# plot some results\n","fig,ax = plt.subplots(1,2,figsize=(17,7))\n","\n","ax[0].plot(accuracyResultsTrain)\n","ax[0].set_title('Train accuracy')\n","ax[1].plot(accuracyResultsTest)\n","ax[1].set_title('Test accuracy')\n","\n","# common features\n","for i in range(2):\n","  ax[i].legend(batchsizes)\n","  ax[i].set_xlabel('Epoch')\n","  ax[i].set_ylabel('Accuracy (%)')\n","  ax[i].set_ylim([50,100])\n","  ax[i].grid()\n","\n","plt.show()\n","\n","\n","\n","# bar plot of computation time\n","plt.bar(range(len(comptime)),comptime,tick_label=batchsizes)\n","plt.xlabel('Mini-batch size')\n","plt.ylabel('Computation time (s)')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UMTFQTPqBjr"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"b-DYWPmfTfZR"},"source":["# Additional explorations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRokrXL9Thv7"},"outputs":[],"source":["# 1) There is another regularization technique called \"early stopping,\" which simply means to stop training the model\n","#    earlier than the number of epochs you specified. Early stopping is used when the test accuracy starts to decrease\n","#    with increased training. Do you think that early stopping would be beneficial here? How many epochs would you train?\n","# \n","# 2) The training loop computes the losses, but those aren't plotted. Create an additional subplot to show the losses\n","#    in a similar fashion as the accuracy. Does that plot provide any additional insights into the effects of minibatch\n","#    size, beyond what we can already learn from the accuracy plots.\n","# "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPA3Kz5KpktWjUBlXjMqELG","collapsed_sections":[],"name":"DUDL_metaparams_CodeChallengeBatches.ipynb","provenance":[{"file_id":"1ZD_ADbh6qrlHE16V7Yc8VF9Vrn2c6bMQ","timestamp":1617088339885}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
